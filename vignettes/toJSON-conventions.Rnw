%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Conventions for Encoding Common R data Structures in JSON}

\documentclass{article}
\author{Jeroen Ooms}
\title{Conventions for Encoding Common R data Structures in JSON}

\usepackage{url}
\usepackage{fullpage}

\begin{document}

\maketitle

<<echo=FALSE>>=
library(JSONlite)
@

\section{Introduction}

JSON (JavaScript Object Notation) is a lightweight data-interchange format, and one of the most common methods today for exchange of simple data structures in systems and applications. It is easy for humans to read and write, and easy for machines to parse and generate. The JSON format itself is very simple and completely defined in a single page at \url{http://www.json.org/}. This simplicity allows for directly mapping some universal data structures from any programming language into JSON strings, and vice versa, which is convenient for developers and has contributed to the popularity of the format. Several R packages including \texttt{RJSONIO}, \texttt{rjson} and \texttt{JSONlite} implement some sort of \texttt{toJSON} and \texttt{fromJSON} functions which convert between basic R data structures and JSON.  However, the intuitive mapping in R is limited to lists and vectors; there is no consensus on how some of the other common classes in R, such as factors, matrices and data frames should be mapped to JSON. Furthermore, upon closer inspection, even the universal data structures in R actually do not perfectly map to their JSON counterparts, and leave some ambiguity for edge cases such as missing values and vectors of length 0 or 1. These problems have resulted in different behavior between the packages mentioned earlier, and can cause unexpected output for certain special cases, which affects reliability of applications that rely on JSON to get data in and out of R. This document tries to take away some of the ambiguity by describing the mapping a more formally and explicitly, highlighting problems and propose conventions that can generalize the mapping to cover all common classes and cases in R.

\subsection{Reference implementation: the \texttt{JSONlite} package}

The \texttt{JSONlite} package provides a reference implementation of the conventions listed in this document. \texttt{JSONlite} is a fork of the \texttt{RJSONIO} package by Duncan Temple Lang, which again builds on \texttt{libjson} \texttt{C++} library. The \texttt{JSONlite} package uses the parser code from \texttt{RJSONIO}, but the R code has been rewritten from scratch. Both packages implement similar \texttt{toJSON} and \texttt{fromJSON} functions, but their output can be quite different. Finally, the \texttt{JSONlite} package contains a large set of unit tests to validate that R objects are correctly converted to JSON and vice versa. Any other implementation of the conventions listed in this document can be validated by performing the same unit tests.

<<eval=FALSE>>=
library(testthat)
test_package("JSONlite")
@

\noindent Note that evem though \texttt{JSON} allows for inserting arbitrary whitespace and indentation, the unit tests assume that all whitespace is trimmed.

\subsection{Class-based versus type-based encoding}

The \texttt{JSONlite} package actually implements two methods for translating between R objects and JSON. This document focuses on the \texttt{toJSON} and \texttt{fromJSON} functions which use R's class-based S3 method system. For all of the common classes in R, the \texttt{JSONlite} package implements a \texttt{toJSON} method according to the conventions outlined in this document. Users in R can easily extend this system by implementing additional \texttt{toJSON} methods for other classes. However this also means that classes that do not have the \texttt{toJSON} method defined are not supported. Furthermore, the implementation of a specific \texttt{toJSON} method determintes which data and metadata in the objects of this class gets encoded in its JSON representation, and how. In this respect, \texttt{toJSON} is similar to e.g. the \texttt{print} function, which also provides a certain \emph{representation} of an object based on its class and optionally some print parameters. This representation does not necessarily contain all information stored in the object. As a result, there is no one-to-one correspondence between R objects and JSON strings. I.e. calling \texttt{fromJSON(toJSON(object))} might return an object which only contains the data that was encoded by the \texttt{toJSON} method for this particular class of object, and is not identical to the original object. \\

The alternative is to use type-based encoding, which \texttt{JSONlite} does in the functions \texttt{serializeJSON} and \texttt{unserializeJSON}. All data structures in R are implemented using a limited number of internal \texttt{SEXP} types, and \texttt{serializeJSON} defines an encoding schema which captures the type, value, and attributes for each object. The result is JSON output which closely resembles the internal structure of the underlying C data types, and which can be perfectly restored to the original R object using \texttt{unserializeJSON}. This method is relatively straightforward to implement, however the disadvantage is that the resulting JSON structure can be very verbose, hard to interpret and cumbersome to generate in the context of another language or system. For most applications this is actually unpractical. Usually, we can make data in R objects more accessible to third parties by defining sensible JSON encoding based on the object class, rather than based on the internal storage type. In this document we will not treat \texttt{serializeJSON} in any further detail, and limit ourselves to the class based \texttt{toJSON} and \texttt{fromJSON}.

\subsection{Scope and Limitations}

Before continuing, it is important to be aware of the limitations of representing R data structures in JSON. First of all, there are the general limitations to types of objects that can be serialized. Temporary in-memory properties such as connections, file descriptors and (recursive) memory references are always difficult if not impossible to store in a sensible way, regardless of the language or serialization method. This document mostly focusses on R classes that hold  data that can be exchanged with other languages, e.g. vectors, lists, dates, matrices, data frames, etc. We are not overly interested in language level constructs such as expressions, functions, promises, which hold little meaning outside the context of R. Then there are limitations in the format itself. JSON is a human readable, text-based format which does not support binary data, and numbers are stored in their decimal representation. This leads to loss of precision for real numbers, depending on how many digits the user decides to print. Finally, as mentioned earlier, the class based S3 method system allows for consise and practical encoding of the various data structures, but makes that \texttt{fromJSON} is not a perfect inverse function of \texttt{toJSON}, as is the case for e.g. \texttt{serialializeJSON} and \texttt{unserializeJSON}. For example, the JSON reperesentation of an empty vector, empty list or empty data frame are all the same: \texttt{"[ ]"}. In the design of the functions we target consistent behavior, and try to adhere to the principle of being strict on output and tolerant on input. 

\section{Converting R objects to JSON}
 
This section lists options and examples of representing the common R classes in JSON, i.e. the \texttt{toJSON} function in \texttt{JSONlite}. As explained before, we rely on the class based S3 method system in R; hence objects get encoded according to their \texttt{class} value. If an object has multiple \texttt{class} values, R will use the first occuring class which has a \texttt{toJSON} method implemented. If none of the classes of an object has a \texttt{toJSON} method, R will raise an error.

\subsection{Atomic Vectors and Missing Values}

The most basic data type in R is the atomic vector. The atomic vector holds an ordered set of single values of the same type, where the type is one of \texttt{"logical"} (booleans), \texttt{integer}, \texttt{numeric} (doubles), \texttt{character} (strings), or \texttt{"raw"} (bytes). In addition, most types can hold "missing values". R is completely vectorized, and has no user level notion of a scalar. A vector can be of length 1, but it will still be a vector. When encoding to JSON, atomic vectors become JSON arrays:

<<>>=
x <- c(1, 2, pi)
cat(toJSON(x))
@

\noindent The JSON array is the most natural way of encoding an R vector, however the JSON array is actually more general than the R vector. In contrast to a atomic vector, a JSON array can hold values of different types, and therefore is conceptually actually closer to a list in R. 

\subsubsection{Missing Values}

A typical domain specific problem when working with statistical data is how to deal with missing values: a concept foreign to many other languages. The R vector types \texttt{logical}, \texttt{integer}, \texttt{character} and \texttt{double} can hold \texttt{NA} as a value. Vectors of type \texttt{double} can have three additional types of non finite values: \texttt{NaN}, \texttt{Inf} and \texttt{-Inf}. JSON does not have a native NA type, so these values need to be encoded in some other way. There are two ways to go about this. The first one is to use the native JSON \texttt{null} type. For example:

<<>>=
x <- c(TRUE, FALSE, NA)
cat(toJSON(x))
@

\noindent The other option is to encode missing values as strings by wrapping them in double quotes:

<<>>=
x <- c(1,2,NA,NaN,Inf,10)
cat(toJSON(x))
@

\noindent Both methods have a limitation: the problem with the \texttt{null} type is that there is no way to distinguish between different types of missing values, which is a problem for numeric vectors. The values \texttt{Inf}, and \texttt{NA} have a very different meaning, and we don't want this to get lost in the encoding. The problem with encoding missing values as strings is that this method can not be used for character vectors, because we won't be able to distinguish bewteen the actual string "NA" and the missing value NA. For this reason, \texttt{JSONlite} uses the following defaults:

\begin{itemize}
 \item Missing values in non-numeric vectors (\texttt{logical}, \texttt{character}) are encoded as \texttt{null} in JSON.
 \item Missing values in numeric vectors (\texttt{double}, \texttt{integer}, \texttt{complex}) are encoded as strings.
\end{itemize}

\noindent Some examples:

<<>>=
cat(toJSON(c(TRUE, NA, NA, FALSE)))
cat(toJSON(c("FOO", "BAR", NA, "NA")))
cat(toJSON(c(3.14, NA, NaN, 21, Inf, -Inf)))
cat(toJSON(c(3.14, NA, NaN, 21, Inf, -Inf), NA_as_string=FALSE))
@

\subsubsection{Special vector types: dates, times, factor, complex}

Another domain specific problem is presented by common classes which are considered low-level in R, but have no native JSON type. These include dates, POSIX timestamps, factors and complex vectors. In this respect, JSON is subject to the same limitations as text based formats such as CSV. By default, the \texttt{JSONlite} package encodes these types to strings:

<<>>=
cat(toJSON(Sys.time() + 1:3))
cat(toJSON(as.Date(Sys.time()) + 1:3))
cat(toJSON(factor(c("foo", "bar", "foo"))))
cat(toJSON(complex(real=runif(3), imaginary=rnorm(3))))
@

\noindent When parsing such JSON using \texttt{fromJSON}, these arrays will turn into character vectors. In order to get back the original types, the user will have to use \texttt{as.POSIXct}, \texttt{as.Date}, \texttt{as.factor} or \texttt{as.complex}, etc.


\subsubsection{Special cases: vectors of length 0 or 1}

Two special cases deserve special attention: vectors of length 0 and vectors of length 1. In \texttt{JSONlite} these are encoded respectively as an empty array, and an array of length 1:

<<>>=
#vectors of length 0 and 1
cat(toJSON(vector()))
cat(toJSON(pi))

#vectors of length 0 and 1 in a named list
cat(toJSON(list(foo=vector())))
cat(toJSON(list(foo=pi)))

#vectors of length 0 and 1 in an unnamed list
cat(toJSON(list(vector())))
cat(toJSON(list(pi)))
@

\noindent This might seem trivial but these special cases have very different behavior between different JSON packages. For example, in the current implementations, both \texttt{RJSONIO} and \texttt{rjson} encode a vector of length one as a primitive when it appears within a list: 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Other packages make different choices:}
\hlkwd{cat}\hlstd{(rjson::}\hlkwd{toJSON}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## {"n":1}
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(rjson::}\hlkwd{toJSON}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## {"n":[1,2]}
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent When encoding a single dataset, this might seem harmless, but in the context of dynamic data this is inconsisent and prone to cause bugs. For example, imagine an R web service which allows the user to fit a linear model on some data. The R server returns a vector of coefficients encoded in JSON, and the client code then parses the JSON, and then iterates over the array of coefficients to display them in a GUI. All goes well, until the user decides to fit a model with only one predictor, and the application breaks. What happened? The JSON encoder is returning a primitive value, where the client is assuming an array. Unless the client is aware of this special case and has explicitly taken this exception into account, it will proceed as usual and call an interator or loop method on a primitive object, resulting in the obvious errors. For this reason \texttt{JSONlite} uses consistent encoding schemes which do not depend on variable object properties such as its length.

\subsection{Lists}

For the purpose of JSON encoding, two types of lists are distinguished: named lists and unnamed lists. A list in R is considered a named list if it has an attribute called \texttt{"names"}. In practice, a named list is any list for which we can access an element by its name, whereas an unnamed list the elements can only be accessed by their index number:

<<>>=
mylist1 <- list("foo" = 123, "bar"= 456)
print(mylist1$bar)

mylist2 <- list(123, 456)
print(mylist2[[2]])
@

\subsubsection{Unnamed lists}

The unnamed list is encoded using a JSON array:

<<>>=
cat(toJSON(list(c(1,2), "test")))
cat(toJSON(list(c(1,2), list(c(1,2)))))
@

\noindent Note that both vectors and lists are now encoded using JSON arrays, but we can still distinguish them from their contents: a JSON array which contains primitives is an R vector, whereas a JSON array which contains objects and arrays is a list. For this reason parsing the JSON back in R will result in the correct type in most cases:

<<>>=
x <- list(c(1,2,NA), "test", FALSE, list(foo="bar"))
identical(fromJSON(toJSON(x)), x)
@

\subsubsection{Named lists}

A named list in R gets encoded as a JSON \emph{object}:

<<>>=
cat(toJSON(list(foo=c(1,2), bar="test")))
@

\noindent The named list almost perfectly maps to a JSON object with one exception: R list elements can have empty names:

<<>>=
x <- list(foo=123, "test", TRUE)
attr(x, "names")
x$foo
x[[2]]
@

\noindent In a JSON object, each element in an object must have a valid name. We use the same solution as the \texttt{print} method for lists which is to fall back on indices for elements that do not have a proper name:

<<>>=
x <- list(foo=123, "test", TRUE)
cat(toJSON(x))
@


\subsection{Data frames}



\section{Converting JSON to R objects}

Guidelines:

 - try to recover orriginal object
 - at least as.foo(x) should give very similar object.


\end{document}