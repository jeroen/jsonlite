%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{A Practical and Consistent Mapping Between Common R Data Structures to JSON and JSON}

%shorthands
\newcommand{\JSON}{\texttt{JSON}\xspace}
\newcommand{\toJSON}{\texttt{toJSON}\xspace}
\newcommand{\fromJSON}{\texttt{fromJSON}\xspace}
%meta
\documentclass{article}
\author{Jeroen Ooms}
\title{A Practical and Consistent Mapping Between JSON Data and Common Object Classes in The R Language}

%some packages
\usepackage{url}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{booktabs}

%extra line spacing
\usepackage{xspace}
\setstretch{1.2}

%actual doc
\begin{document}

\maketitle

<<echo=FALSE>>=
library(JSONlite)
opts_chunk$set(comment="")

#this replaces tabs by spaces because latex-verbatim doesn't like tabs
toJSON <- function(...){
  gsub("\t", "  ", JSONlite::toJSON(...), fixed=TRUE);
}
@

\section{Introduction}

JSON (JavaScript Object Notation) is a lightweight data-interchange format, and one of the most common methods today for exchange of data structures in systems and applications. It is easy for humans to read and write, and easy for machines to parse and generate. The syntax is completely defined in a single page at \url{http://www.json.org/}: it specifies 5 primitive types (\texttt{string}, \texttt{number}, \texttt{true}, \texttt{false}, \texttt{null}) and two \emph{universal data structures}:

\begin{itemize}
  \item A collection of name-value pairs. In R, this is realized as a \emph{named list}.
  \item An ordered set of values. In R, this is realized as an \emph{unnamed list}.
\end{itemize}

\noindent Because the syntax is limited to these universal types and structures, \JSON data is easily converted into a native object in almost any programming language. This is convenient for developers and has contributed to the popularity of the format. Several R packages already implement \toJSON and \fromJSON functions which convert between R objects and \JSON structures. However, the exact mapping between the various R classes and the two universal data structures that are supported by \JSON is not self evident. Currently, there are no formal guidelines, or even consensus between implementations on how exactly each class in R should be represented in \JSON. Furthermore, upon closer inspection, even the most basic data structures in R actually do not perfectly map to their \JSON counterparts, and leave some ambiguity for edge cases. These problems have resulted in different behavior between implementations, and can lead to unexpected output for special cases. To further complicate things, best practices of representing data in \JSON have already been established outside the R community. Respecting these conventions where possible is important to maximize interoperability of data in R when using \JSON.

When relying on \JSON as the data interchange format, the mapping between R objects and \JSON data must be consistent and unambiguous. Clients relying on \JSON to get data in and out of R must know exactly what to expect in order to facilitate reliable communication, even if the data themselves are dynamic. Similarly, R code using dynamic \JSON data from an external source is only reliable when the conversion from \JSON to R is consistent. This document attempts to take away some of the ambiguity by explicitly describing the mapping between R classes and \JSON data, highlighting problems and propose conventions that can generalize the mapping to cover all common classes and cases in R.

\subsection{Reference implementation: the \texttt{JSONlite} package}

The \texttt{JSONlite} package provides a reference implementation of the conventions proposed in this document. \texttt{JSONlite} is a fork of the \texttt{RJSONIO} package by Duncan Temple Lang, which again builds on \texttt{libjson} \texttt{C++} library from Jonathan Wallace. The \texttt{JSONlite} package uses the parser from \texttt{RJSONIO}, but the R code has been rewritten from scratch. Both packages implement \toJSON and \fromJSON functions, but their output is quite different. Finally, the \texttt{JSONlite} package contains a large set of unit tests to validate that R objects are correctly converted to \JSON and vice versa. These unit tests cover all classes and edge cases mentioned in this document, and could be used to validate if other implementations follow the same convetions. 

<<eval=FALSE>>=
library(testthat)
test_package("JSONlite")
@

\noindent Note that even though \texttt{JSON} allows for inserting arbitrary white space and indentation, the unit tests assume that white space is trimmed.

\subsection{Class-based versus type-based encoding}

The \texttt{JSONlite} package actually implements two systems for translating between R objects and \JSON. This document focuses on the \toJSON and \fromJSON functions which use R's class-based S3 method system. For all of the common classes in R, the \texttt{JSONlite} package implements \toJSON methods as described in this document. Users in R can easily extend this system by implementing additional \toJSON methods for other classes. However this also means that classes that do not have the \toJSON method defined are not supported. Furthermore, the implementation of a specific \toJSON method determines which data and metadata in the objects of this class gets encoded in its \JSON representation, and how. In this respect, \toJSON is similar to e.g. the \texttt{print} function, which also provides a certain \emph{representation} of an object based on its class and optionally some print parameters. This representation does not necessarily reflect all information stored in the object, and there is no guaranteed one-to-one correspondence between R objects and \JSON. I.e. calling \texttt{fromJSON(toJSON(object))} will return an object which only contains the data that was encoded by the \toJSON method for this particular class, and which might even have a different class than the original.

The alternative to the S3 method system is to use type-based encoding, which \texttt{JSONlite} implements in the functions \texttt{serializeJSON} and \texttt{unserializeJSON}. All data structures in R get stored in memory using one of the internal \texttt{SEXP} storage types, and \texttt{serializeJSON} defines an encoding schema which captures the type, value, and attributes for each storage type. The result is \JSON output which closely resembles the internal structure of the underlying C data types, and which can be perfectly restored to the original R object using \texttt{unserializeJSON}. This method is relatively straightforward to implement, however the disadvantage is that the resulting \JSON is very verbose, hard to interpret, and cumbersome to generate in the context of another language or system. For most applications this is actually impractical because it requires the client/consumer to understand and manipulate R types, which is difficult and reduces interoperability. Instead we can make data in R more accessible to third parties by defining sensible \JSON representations that are natural for the class of an object, rather than its internal storage type. This document does not discuss the \texttt{serializeJSON} system in any further detail, and solely treats the class based system implemented in \toJSON and \fromJSON. However the reader that is interested in full serialization of R objects into \JSON is encouraged to have a look at the respective manual pages.

\subsection{Scope and Limitations}

Before continuing, we want to stress some limitations of encoding R data structures in \JSON. Most importantly, there are the limitations to types of objects that can be represented. In general, temporary in-memory properties such as connections, file descriptors and (recursive) memory references are always difficult if not impossible to store in a sensible way, regardless of the language or serialization method. This document focuses on the common R classes that hold \emph{data}, such as vectors, factors, lists, matrices and data frames. We do not treat language level constructs such as expressions, functions, promises, which hold little meaning outside the context of R. We also don't treat special compound classes such as linear models or custom classes defined in contributed packages. When designing systems or protocols that interact with R, it is highly recommended to stick with the standard data structures for the interface input/output.

Then there are limitations introduced by the format. Because \JSON is a human readable, text-based format, it does not support binary data, and numbers are stored in their decimal notation. The latter leads to loss of precision for real numbers, depending on how many digits the user decides to print. Several dialects of \JSON exists such as \texttt{BSON} or \texttt{MSGPACK}, which extend the format with various binary types. However, these formats are much less popular, less interoperable, and often impractical, precisely because they require binary parsing and abandon human readability. The simplicity of \JSON is what makes it an accessible and widely applicable data interchange format. In cases where it is really needed to include some binary data in \JSON, one can use something like \texttt{base64} to encode it as a string.

Finally, as mentioned earlier, \fromJSON is not a perfect inverse function of \toJSON, as would be the case for \texttt{serialializeJSON} and \texttt{unserializeJSON}. The class based S3 method system allows for concise and practical encoding of the various common data structures. In our design of \toJSON and \fromJSON, we have tried to approach a reversible mapping between R objects and \JSON for the standard data classes, but there are always limitations and edge cases. For example, the \JSON representation of an empty vector, empty list or empty data frame are all the same: \texttt{"[ ]"}. Also some special vector types such as factors, dates or timestamps get coersed to strings, as they would in for example \texttt{CSV}. This is a quite typical and expected behavior among text based formats, but it does require some additional interpretation on the side of the consumer.

\subsection{Goals: Consistent and Practical}

It can be helpful to see the problem from both sides. The R user needs to interface external \JSON data from within R. This includes reading data from a public source/API, or posting a specific \JSON structure to an online service. From perspective of the R user, \JSON data should be realized in R using classes which are most natural in R for a particular structure. A proper mapping is one which allows the R user to read any incoming data or generate a specific \JSON structures using the familiar methods and classes in R. Ideally, the R user would like to forget about the interchange format at all, and think about the external data interface in terms of its corresponding R structures rather than a \JSON schema. The other perspective is that of an third party client or language, which needs to interface data in R using \JSON. This actor wants to access and manipulate R objects via their \JSON representation. A good mapping is one that allows a 3rd party client to get data in and out of R, without necessarily understanding the specifics of the underlying R classes. Ideally, the external client could forget about the R objects and classes at all, and think about input and output of data in terms of the \JSON schema, or the corresponding realization in the language of the client. 

Both sides come together in the context of an RPC service such as OpenCPU. OpenCPU exposes a HTTP API to let 3rd party clients call R functions over HTTP. The function arguments are posed using \JSON and OpenCPU automatically converts these into R objects to construct the R function call. The return value of the function is then converted to \JSON and sent back to the client. To the client, the RPC looks like a \JSON API, whereas the R side can simply be implemented as standard R function that takes standard data structures as its arguments and return value. For this to work, the conversion between \JSON data and R objects must be consistent and unambiguous. In the design of our mapping we have tried to pursue the following requirements:

\begin{itemize}
  \item{Recognize and comply with existing conventions of encoding common data structures in \JSON, in particular (relational) data sets.}
  \item{Consistently use a particular schema for a class of objects, including edge cases.}
  \item{Avoid R-specific peculiarities to minimize opportunities for misinterpretation.}
  \item{Mapping should optimally be reversible, but at least coercible.}
  \item{Robustness principle: be strict on output but tolerant on input.}
\end{itemize}


\section{Converting between \JSON and R Classes}
 
This section lists examples of how the common R classes are represented in \JSON. As explained before, the \toJSON method relies on the S3 method system; hence objects get encoded according to their \texttt{class}. If an object has multiple \texttt{class} values, R will use the first occurring class which has a \toJSON method implemented. If none of the classes of an object has a \toJSON method, R will raise an error.

\subsection{Atomic Vectors}

The most basic data type in R is the atomic vector. The atomic vector holds an ordered set of homogeneous values of type \texttt{"logical"} (booleans), \texttt{integer}, \texttt{numeric} (doubles), \texttt{character} (strings), or \texttt{"raw"} (bytes). Because R is fully vectorized, there is no user level notion of a primitive: any scalar value is considered a vector of length 1. Atomic vectors are encoded using a \JSON array:

<<>>=
x <- c(1, 2, pi)
cat(toJSON(x))
@

\noindent The \JSON array is the most natural way of encoding an R vector, however note that vectors in R are homogeneous, whereas the \JSON array is actually heterogeneous, but \JSON does not make this distinction.

\subsubsection{Missing Values}

A typical domain specific problem when working with statistical data is presented by missing data: a concept foreign to many other languages. Besides regular values, vector types \texttt{logical}, \texttt{integer}, \texttt{character} and \texttt{double} can hold \texttt{NA} as a value. Vectors of type \texttt{double} define three additional types of non finite values: \texttt{NaN}, \texttt{Inf} and \texttt{-Inf}. \JSON does not natively support any of these types; therefore such values values need to be encoded in some other way. There are two obvious approaches. The first one is to use the  \JSON \texttt{null} type. For example:

<<>>=
x <- c(TRUE, FALSE, NA)
cat(toJSON(x))
@

\noindent The other option is to encode missing values as strings by wrapping them in double quotes:

<<>>=
x <- c(1,2,NA,NaN,Inf,10)
cat(toJSON(x))
@

\noindent Both methods result in valid \JSON, but both have a limitation: the problem with the \texttt{null} type is that there is no way to distinguish between different types of missing data, which is a problem for numeric vectors. The values \texttt{Inf}, \texttt{-Inf}, \texttt{NA} and \texttt{NaN} have a very different meanings, and these should not get lost in the encoding. The problem with encoding missing values as strings is that this method can not be used for character vectors, because the consumer won't be able to distinguish the actual string \texttt{"NA"} and the missing value \texttt{NA}. This would create a likely source of bugs, where clients mistakenly interpret \texttt{"NA"} as an actual value, which is a common problem with text-based formats such as CSV. For this reason, \texttt{JSONlite} uses the following defaults:

\begin{itemize}
 \item Missing values in non-numeric vectors (\texttt{logical}, \texttt{character}) are encoded as \texttt{null}.
 \item Missing values in numeric vectors (\texttt{double}, \texttt{integer}, \texttt{complex}) are encoded as strings.
\end{itemize}

\noindent Some examples:

<<>>=
cat(toJSON(c(TRUE, NA, NA, FALSE)))
cat(toJSON(c("FOO", "BAR", NA, "NA")))
cat(toJSON(c(3.14, NA, NaN, 21, Inf, -Inf)))
cat(toJSON(c(3.14, NA, NaN, 21, Inf, -Inf), NA_as_string=FALSE))
@

\subsubsection{Special vector types: dates, times, factor, complex}

Besides missing values, \JSON also lacks native support for some of the basic vector types in R that frequently appear in data sets. These include vectors of class \texttt{Date}, \texttt{POSIXt} (timestamps), \texttt{factors} and \texttt{complex} vectors. By default, the \texttt{JSONlite} package coerces these types to strings (using \texttt{as.character}):

<<>>=
cat(toJSON(Sys.time() + 1:3))
cat(toJSON(as.Date(Sys.time()) + 1:3))
cat(toJSON(factor(c("foo", "bar", "foo"))))
cat(toJSON(complex(real=runif(3), imaginary=rnorm(3))))
@

\noindent When parsing such \JSON strings, these values will appear as character vectors. In order to obtain the original types, the user needs to manually coerce them back to the desired type using e.g. \texttt{as.POSIXct}, \texttt{as.Date}, \texttt{as.factor} or \texttt{as.complex}. In this respect, \JSON is subject to the same limitations as text based formats such as CSV. 

\subsubsection{Special cases: vectors of length 0 or 1}

Two special cases deserve special attention: vectors of length 0 and vectors of length 1. In \texttt{JSONlite} these are encoded respectively as an empty array, and an array of length 1:

<<>>=
#vectors of length 0 and 1
cat(toJSON(vector()))
cat(toJSON(pi))

#vectors of length 0 and 1 in a named list
cat(toJSON(list(foo=vector())))
cat(toJSON(list(foo=pi)))

#vectors of length 0 and 1 in an unnamed list
cat(toJSON(list(vector())))
cat(toJSON(list(pi)))
@

\noindent This might seem trivial but these cases result in very different behavior between different \JSON packages, which is probably caused by the fact that R does not distinguish between a scalar and a vector of length 1. For example, in the current implementations, both \texttt{RJSONIO} and \texttt{rjson} encode a vector of length one as a \JSON primitive when it appears within a list: 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Other packages make different choices:}
\hlkwd{cat}\hlstd{(rjson::}\hlkwd{toJSON}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## {"n":1}
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(rjson::}\hlkwd{toJSON}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{n} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## {"n":[1,2]}
\end{verbatim}
\end{kframe}
\end{knitrout}

\noindent When encoding a single data set this might seem harmless, but in the context of dynamic data this inconsistency is almost guaranteed to cause bugs. For example, imagine an R web service which lets the user fit a linear model and sends back the fitted parameter estimates. In this application, the R server returns the vector with coefficients encoded as a \JSON array. The client code then parses the \JSON, and then iterates over the array of coefficients to display them in a GUI. All goes well, until the user decides to fit a model with only one predictor. If the \JSON encoder returns a primitive value where the client is assuming an array, the application will likely break. Any consumer or client would need to be aware of the special case where the vector becomes a primitive, and explicitly take this exception into account when processing the result. When the client fails to do so and proceeds as usual, it will probably call an iterator or loop method on a primitive value, resulting in the obvious errors. For this reason \texttt{JSONlite} uses consistent encoding schemes which do not depend on variable object properties such as its length. Hence, a vector is always encoded as an array, even when it is of length 0 or 1.

\subsection{Matrices}

Arguably one of the strongest sides of R is its ability to interface libraries for basic linear algebra subroutines (BLAS) such as LAPACK, OpenBLAS, etc. These libraries provide well tuned, high performance implementations of important linear algebra operations to calculate anything from inner products and eigen values to singular value decompositions. These are in turn the building blocks of many higher level statistical methods such as linear regression or principal component analysis. Linear algebra methods operate on \emph{matrices}, making the matrix one of the most central data classes in R. Conceptually, a matrix consists of a 2 dimensional structure of homogeneous values. It is indexed using 2 numbers (or vectors), representing the row and column number of the matrix respectively. 

<<>>=
x <- matrix(1:12, nrow=3, ncol=4)
print(x)
print(x[2,4])
@

\noindent A matrix is stored in memory as a single atomic vector with an attribute called \texttt{"dim"} defining the dimensions of the matrix. The product of the dimensions is equal to the length of the vector. 

<<>>=
attributes(volcano)
length(volcano)
@

\noindent Even though the matrix is stored as a single vector, it the way it is printed, indexed and treated otherwise makes it conceptually closer to 2 dimensional structure. \texttt{JSONlite} treats a matrix as a set of equal-length homogeneous vectors, and therefore encodes it using an array of sub arrays:

<<>>=
x <- matrix(1:12, nrow=3, ncol=4)
cat(toJSON(x))
@

\noindent We hope that this represention will be the most intuitive to interpret, even within languages that do not have a native notion of a matrix. When the JSON string is properly indented (recall that whitespace and linebreaks are optional in JSON), it looks very similar to the way R prints matrices:

\begin{verbatim}
[ [ 1, 4, 7, 10 ], 
  [ 2, 5, 8, 11 ], 
  [ 3, 6, 9, 12 ] ]
\end{verbatim}

\noindent Because the matrix is implemented in R as an atomic vector, it automatically inherits the conventions mentioned earlier with respect to edge cases and missing values:

<<>>=
x <- matrix(c(1,2,4,NA), nrow=2)
cat(toJSON(x))
cat(toJSON(x, NA_as_string=FALSE))
cat(toJSON(matrix(pi)))
@


\subsubsection{Matrix row and column names}

Besides the \texttt{"dim"} attribute, matrices in R can have another attribute: \texttt{"dimnames"}. This attribute holds names for the rows and columns in the matrix. However, we decided not to include this information in the default \JSON encoding for matrices for several reasons. First of all, for objects of class matrix the dimnames attribute is optional and often either row or column names or both are \texttt{NULL}. This makes it difficult to define a practical encoding that covers all cases with and without row and/or column names. Secondly, the names in matrices are mostly there for annotation only; they are not actually used in calculations. For example, the linear algebra operations mentioned before completely ignore them, and do not include any names in their output. So when using matrices for linear algebra, there is often little purpose of setting names in the first place. 

When row and column names of a matrix contain vital information, chances are they are actually variable values themselves, and the matrix is not the most appropriate structure to store this data. It might be useful to transform the data into a more appropriate structure. Wickham (2013) calls this tidying the data and outlines best practices on structuring data in. The example below is taken from Wickham (2013). A matrix structured below is practical for printing, but not for manipulating the data. The main variable, treatment, is put in the header:

<<>>=
x <- matrix(c(NA,1,2,5,4,3), nrow=3)
row.names(x) <- c("Joe", "Jane", "Mary");
colnames(x) <- c("Treatment A", "Treatment B")
print(x)
@

\noindent In this case might want to melt the data, before converting it to \JSON:

<<>>=
library(reshape2)
y <- melt(x, varnames=c("Subject", "Treatment"))
print(y)
@

\noindent Tidy date will most likely result in a practical \JSON representation. Compare the encoding of the original and tidy data:

<<>>=
cat(toJSON(x))
cat(toJSON(as.data.frame(x), pretty=TRUE))
cat(toJSON(y, pretty=TRUE))
@

For data sets with records consisting of a set of named columns (fields), R has more natural and flexible class: the data-frame. The \toJSON method for data-frames is described below and is much suitable when we want to refer to rows or fields by a particular name. Any matrix can easily be converted to a dataframe using the \texttt{as.data.frame} function.


\subsection{Lists}

The \texttt{list} is the most general purpose data structure in R. It holds an ordered set of elements, including other lists, each of arbitrary type and size. For the purpose of \JSON encoding, two types of lists are distinguished: named lists and unnamed lists. A list is considered a named list if it has an attribute called \texttt{"names"}. In practice, a named list is any list for which we can access an element by its name, whereas elements of an unnamed lists can only be accessed using their index number:

<<>>=
mylist1 <- list("foo" = 123, "bar"= 456)
print(mylist1$bar)
mylist2 <- list(123, 456)
print(mylist2[[2]])
@

\subsubsection{Unnamed lists}

Just like vectors, an unnamed list is encoded using a \JSON array:

<<>>=
cat(toJSON(list(c(1,2), "test", TRUE, list(c(1,2)))))
@

\noindent Note that even though both vectors and lists are encoded using \JSON arrays, they can be distinguished by looking at their contents: an R vector results in a \JSON array which contains only primitives, whereas a list results in a \JSON array which contains only objects and arrays. This allows the \JSON parser to reconstruct the original type from encoded vectors and arrays:

<<>>=
x <- list(c(1,2,NA), "test", FALSE, list(foo="bar"))
identical(fromJSON(toJSON(x)), x)
@

\noindent The only exception is the empty list and empty vector, which are both encoded as \texttt{[ ]} and therefore indistinguishable, but this is rarely a problem in practice. 

\subsubsection{Named lists}

A named list in R gets encoded as a \JSON \emph{object}:

<<>>=
cat(toJSON(list(foo=c(1,2), bar="test")))
@

\noindent Because a list can contain other lists, this works recursively:

<<tidy=FALSE>>=
cat(toJSON(list(foo=list(bar=list(baz=pi)))))
@

\noindent Named lists map almost perfectly to \JSON objects with one exception: list elements can have empty names:

<<>>=
x <- list(foo=123, "test", TRUE)
attr(x, "names")
x$foo
x[[2]]
@

\noindent In a \JSON object, each element in an object must have a valid name. To ensure this property, \texttt{JSONlite} uses the same solution as the \texttt{print} method, which is to fall back on indices for elements that do not have a proper name:

<<>>=
x <- list(foo=123, "test", TRUE)
print(x)
cat(toJSON(x))
@


\subsection{Data frame}

The \texttt{data frame} is perhaps the most central data structure in R from the user point of view. This class holds a tabular data structure in which each column is named and (usually) homogeneous. Conceptually it is very similar to a table in relational data bases such as \texttt{MySQL}, where \emph{fields} are referred to as \emph{column names}, and \emph{records} are called \emph{row names}. Like a matrix, a data frame can be subsetted with two indices, to extract certain rows and columns of the data:

<<>>=
is(iris)
names(iris)
print(iris[1:3, c(1,5)])
print(iris[1:3, c("Sepal.Width", "Species")])
@

\subsubsection{Column based versus row based tables}

Generally speaking, tabular data structures can be implemented in two different ways: in a column based, or row based fasion. A column based structure consists of a named collection of equal-length, homogenous arrays representing the table columns. In a row-based structure on the other hand, the table is implemented as a set of heterogeneous associative arrays representing table rows with field values for each particular record. Even though most languages provide flexible and abstracted interfaces that hide such implementation details from the user, they can have huge implications for performance. A column based structure is efficient for inserting or extracting certain columns of the data, but it is inefficient for manipulating individual rows. For example to insert a single row somewhere in the middle, each of the columns has to be sliced and stiched back together. For row-based implementations, it is the extact other way around: we can easily manipulate a particular record, but to insert/extract a whole column we would need to iteriate over all records in the table and read/modify the appropriate field in each of them.

The data frame in R is implemented in a column based fasion: it constitutes of a \texttt{named list} of equal-length vectors. Thereby the columns in the data frame naturally inherit the properties from atomic vectors discussed before, such as homogeneity, missing values, etc. Another argument for column-based implementation is that statistical methods generally operate on columns. For example, the \texttt{lm} function fits a \emph{linear regression} by extracting the columns from a data frame as specified by the \texttt{formula} argument. R simply binds the specified columns together into a matrix \texttt{X} and calls out to a highly optimized fortran subroutine to calculate the OLS estimates $\hat{\beta} = (X^TX)X^Ty$ using the QR factorization of \texttt{X}. Many other statistical modeling functions follow similar steps, and are computationally efficient because of the column-based data storage in R. However, unfortunately R is an exception in its preference for column-based storage: many other languages, systems, databases, API's, etc are optimized for record based operations. For this reason, the conventional way to store and communicate tabular data in \JSON seems to almost exclusively row based. This discrepancy presents various complications when converting between data frames and \JSON. The remaining of this section discusses details and challenges of consistently mapping record based \JSON data as frequently encountered in the wild, into column-based data frames which are convenient for statistical computing.

\subsubsection{Row based data frame encodingin JSONlite}

The way data frames are encoded is one of the major differences between JSONlite and the \toJSON implementations from other currently available packages. Instead of using the column-based encoding also used for lists, JSONlite encodes data frames by default as an array of records:

<<>>=
cat(toJSON(iris[1:2,], pretty=TRUE))
@

\noindent This output looks a bit like a list of named lists. However, there is one major difference: the individual records contain primitives values, which will never appear in lists (remember, a scalar in R is a vector of length 1):

<<>>=
cat(toJSON(list(list(Species="Foo", Width=21))))
@

\noindent This leads us to the following convention: when encoding R objects, \JSON primitives only appear in vectors and data-frame rows. Primitive values within a \JSON array indicate a vector, and \JSON primitives appearing inside a \JSON object indicate a data-frame row. A \JSON encoded \texttt{list}, (named or unnamed) will never contain \JSON primitives. This is a subtle but important convention that helps to distinguish between R classes from their \JSON resprestation, without explicitly encoding any metadata.

\subsubsection{Missing values in data frames}

The section on atomic vectors discussed two methods of encoding missing data appearing in a vector: either using strings or using the \JSON \texttt{null} type. When a missing value appears in a data frame, there is a third option: simply not include this field in \JSON record:

<<>>=
x <- data.frame(foo=c(FALSE, TRUE,NA,NA), bar=c("Aladdin", NA, NA, "Mario"))
print(x)
cat(toJSON(x, pretty=TRUE))
@

\noindent The default behavior of JSONlite is to omit missing data from records in a data frame. This seems to be the most conventional way of doing things, and we expect this encoding will most likely lead to the correct interpretation of \emph{missingness}, even in languages which have no explicit notion of \texttt{NA}.

\subsubsection{Relational data: nested records}

A structure somewhat unusual in R but frequently encountered in \JSON, is a dataset with records containing nested substructures. Such structures do not really fit the vector based paradigm which makes them hard to manipulate in R, and we should probably avoid them where possible. However, nested records are too common in \JSON data to ignore, and with a little work most cases can still map to a data frame quite nicely. The most common scenario is a dataset in which a certain field within each record contains a \emph{subrecord} with additional fields. JSONlite interprets these subrecords as a nested data frame. Whereas the data frame class usually consists of vectors, technically a column can also be list or another data frame with matching dimension (although this does stretch the meaning of the word "column" a bit):

<<tidy=FALSE>>=
x <- data.frame(driver = c("Bowser", "Peach"), occupation = c("Koopa", "Princess"))
x$vehicle <- data.frame(model = c("Piranha Prowler", "Royal Racer"))
x$vehicle$stats <- data.frame(speed = c(55, 34), weight = c(67, 24), drift = c(35, 32))
str(x)
cat(toJSON(x, pretty=TRUE));
@

\noindent \JSON data containing nested records are often generated from a \emph{relational} database. The \JSON field containing a subrecord is created from a \emph{foreign key} pointing to a record in an external table which were joined into a nested structure. The nested subrecord represents a \emph{one-to-one} or \emph{many-to-one} relation between the parent and child table, and can quite naturally be stored in R using a nested data frame. In the example above, the \texttt{vehicle} field points to a table of verhicles, which in turn contains a \texttt{stats} field pointing to a table of stats. When there is exactly one subrecord for each record, we can decide to \emph{flatten} the structure into a single non-nested data frame.

<<>>=
myjson <- toJSON(x)
y <- fromJSON(myjson)
str(y)
flatdf <- cbind(y[c("driver", "occupation")], x$vehicle["model"], x$vehicle$stats)
str(flatdf)
@

\subsubsection{Relational data: nested tables}

The one-to-one relation discussed above is relatively easy to model in R, because easy records contains exactly one subrecord. Therefore we can use either a nested dataframe, or flatten the data frame. However, things get more difficult when \JSON records contain a field with an array. Such a structure appears when tables have a \emph{one-to-many} relation. A standard textbook example is the relation between authors and titles. In the simple case, the field contains an array of primitives:

<<tidy=FALSE>>=
x <- data.frame(author = c("Homer", "Virgil", "Jeroen"))
x$poems <- list(c("Iliad", "Odyssey"), c("Eclogues", "Georgics", "Aeneid"), vector());
names(x)
cat(toJSON(x, pretty = TRUE))
@

\noindent However in practice, the one-to-many relation often results in fields containing a \emph{set of records}. In R, the only way to model this is as a column containing a list of data frames, one for each row:

<<tidy=FALSE>>=
x <- data.frame(author = c("Homer", "Virgil", "Jeroen"))
x$poems <- list(
  data.frame(title=c("Iliad", "Odyssey"), year=c(-1194, -800)),
  data.frame(title=c("Eclogues", "Georgics", "Aeneid"), year=c(-44, -29, -19)),
  data.frame()
)
cat(toJSON(x, pretty=TRUE))
@

\noindent Because R doesn't have native support for relational data, there is no natural way to model these structures. The best we can do is a column containing a list of data frames. This allows us to access the data, but we can't flatten it. Also this structure does not guarantee that each of these subtables contains the same fields, as would be the case in an actual relational data base. 

\section{Best Practices}

- Unnamed lists containing different structures.
- Tidy data: don't confuse variables and values.
- Array of records with different fields names (i.e. not really a table)


\section{Public JSON APIs}

\section{Converting \JSON to R objects}

Guidelines:

 - when well behaved (arroding to outlined conventions), try to recover original object
 - at least as.foo(x) should give very similar object.
 - 


\end{document}